# include the application.conf at the top
include required(classpath("application"))

system {
  job-rate-control {
    jobs = 1
    per = 1 second
  }
}

backend {
  default = "Slurm"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 200
        # poll for aliveness of job once a minute
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        Int rt_time = 600
        Int cpus = 2
        Int mem = 8000
        String rt_queue = "norm"
        String? docker
        """

        submit = """
            sbatch \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${rt_time} \
              -c ${cpus} \
              --mem ${mem} \
              --partition ${rt_queue} \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Load Singularity into PATH
            module load singularity
            # Add cluster bind paths
            export SINGULARITY_BINDPATH="/vf,/gpfs,/spin1,/data,/scratch,/fdb,/lscratch"

            # SINGULARITY_CACHEDIR needs to point to a directory accessible by
            # the jobs (i.e. not lscratch). Might want to use a workflow local
            # cache dir like in herxv to save on time pulling in the large image
            # should SINGULARITY_CACHEDIR set in hervx (workingDir/.singularity)
            if [ -z $SINGULARITY_CACHEDIR ]; then
                echo "SINGULARITY_CACHEDIR not set... Defaulting to $HOME directory."
                CACHE_DIR=$HOME/.singularity
            else
                CACHE_DIR=$SINGULARITY_CACHEDIR
            fi

            mkdir -p $CACHE_DIR
            LOCK_FILE=$CACHE_DIR/singularity_pull_flock

            # We want to avoid all the cromwell tasks hammering each other trying
            # to pull the container into the cache for the first time. flock works
            # on GPFS, netapp, and vast (of course only for processes on the same
            # machine which is the case here since we're pulling it in the master
            # process before submitting).
            flock --exclusive --timeout 1200 $LOCK_FILE \
                singularity exec --containall docker://${docker} \
                echo "Successfully pulled ${docker}!"

            sbatch \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${rt_time} \
              -c ${cpus} \
              --mem ${mem} \
              --partition ${rt_queue} \
              --wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "dashboard_cli jobs -j ${job_id} | grep -E 'RUNNING|PENDING|COMPLETING'"
        job-id-regex = "(\\d+)"
      }
    }
  }
}
