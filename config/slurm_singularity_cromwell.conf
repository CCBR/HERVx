backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 250
        run-in-background = true
        runtime-attributes = """
        Int runtime_minutes = 60
        Int cpus = 2
        Int mem = 8000
        String queue = "norm"
        String? docker
        String? sif
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${runtime_minutes} \
              -p ${queue} \
              ${"-c " + cpus} \
              --mem=${mem} \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """

            # Load Singularity into PATH
            module load singularity
            export SINGULARITY_BINDPATH="/vf,/gpfs,/spin1,/data,/scratch,/fdb,/lscratch"

            # Make sure the SINGULARITY_CACHEDIR variable is set
            export SINGULARITY_CACHEDIR="/data/CCBR_Pipeliner/db/PipeDB/db/SingularityImages/"

            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${runtime_minutes} \
              -p ${queue} \
              ${"-c " + cpus} \
              --mem=${mem} \
              --wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} ${sif} ${job_shell} ${docker_script}"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}
